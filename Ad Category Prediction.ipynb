{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ниже приведен код без результатов его запуска, т.к. обучение моделей на исходных данных и подбор параметров на доступном мне оборудовании занимает много времени"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификатор объявлений по категориям на основе сверточной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание метода приведено в статье \"Convolutional Neural Networks for Sentence Classification\"\n",
    "http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "\n",
    "Еще одна статья об использовании CNN в обработке текстов:\n",
    "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "\n",
    "Релизация: https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Импортируем основные библиотеки для анализа данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зафиксируем генератор случайных чисел для воспроизводимости результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим исходные данные и просмотрим первые записи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:\\\\Users\\\\al.nikolaev\\\\Desktop\\\\Avito\\\\data\\\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработаем тексты перед их анализом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re # регулярные выражения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer() # морфологический анализатор русского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('russian') # загружаем набор стоп-слов русского языка (самые частотные, не определяют тему)\n",
    "stop_words += [u'купить', u'продать', u'отдать', u'цена', u'метро', u'недорого', u'дешево', u'продаваться', u'новый', \n",
    "               u'самовывоз', u'адрес'] # добавляем слова, типичные для объявлений, но не определяющие тему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.decode('utf-8') # декодируем юникод\n",
    "    text = re.sub(r\"\\s+[^\\s]*[0-9]+[^\\s]*([\\s]+|$)\", \" \", text) # убираем модели, номера, измерения\n",
    "    text = re.sub(ur\"[^ЁёА-Яа-яA-Za-z\\s]\", \" \", text) # убираем все небуквенные символы\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text) # убираем лишние пробелы\n",
    "    text = text.strip() # убираем пробелы в начале и в конце\n",
    "    text = text.lower() # приводим все слова к строчным буквам\n",
    "    text = text.split(\" \") # разбиваем текст на слова\n",
    "    text = [morph.parse(word)[0].normal_form for word in text] # заменяем каждое слово его нормальной формой\n",
    "    text2 = []\n",
    "    for word in text:\n",
    "        if len(word)>1 and not word in stop_words:\n",
    "            text2.append(word) # исключаем из текстов стоп-слова и слова длины 1\n",
    "    text = text2\n",
    "    return text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем набор нормализованных и разбитых на слова текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "    text = train.title[i] + ' ' + train.description[i] \n",
    "    texts.append(normalize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополняем все тексты так, чтобы они содержали одинаковое количество слов (необходимо для обучения нейронной сети)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def equalize_texts(texts, eq_word=\"<eq>\"):\n",
    "    max_len = max(len(x) for x in texts)\n",
    "    eq_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        text = texts[i]\n",
    "        num_to_add = max_len - len(text)\n",
    "        text += [eq_word] * num_to_add\n",
    "        eq_texts.append(text)\n",
    "    return eq_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = equalize_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь всех слов текстов и упорядочим их по частоте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    word_counts = Counter(itertools.chain(*texts)) # счетчик встречаемости слов во всем корпусе текстов\n",
    "    # Словарь для поиска слова по индексу\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()] # записываем слова по убыванию частоты встречаемости\n",
    "    # Словарь для поиска индекса по слову\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab, vocab_inv = build_vocab(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для векторизации слов текстов и сохранения их смысловой нагрузки используем предобученные на корпусе русских текстов векторы Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем готовую функцию для загрузки предобученного на корпусе русских текстов Word2Vec (из документации к файлу)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from numpy import zeros, dtype, float32 as REAL, fromstring\n",
    "from gensim.models.word2vec import Vocab\n",
    "from gensim import utils\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "logger = logging.getLogger(\"gensim.models.word2vec\")\n",
    "\n",
    "def load_vectors(fvec):\n",
    "#    return gs.models.Word2Vec.load_word2vec_format(fvec,binary=True)\n",
    "    return load_word2vec_format(fvec, binary=True)\n",
    "\n",
    "\n",
    "def load_word2vec_format(fname, fvocab=None, binary=False, norm_only=True, encoding='utf8'):\n",
    "    counts = None\n",
    "    if fvocab is not None:\n",
    "        logger.info(\"loading word counts from %s\" % (fvocab))\n",
    "        counts = {}\n",
    "        with utils.smart_open(fvocab) as fin:\n",
    "            for line in fin:\n",
    "                word, count = utils.to_unicode(line).strip().split()\n",
    "                counts[word] = int(count)\n",
    "\n",
    "    logger.info(\"loading projection weights from %s\" % (fname))\n",
    "    with utils.smart_open(fname) as fin:\n",
    "        header = utils.to_unicode(fin.readline(), encoding=encoding)\n",
    "        vocab_size, vector_size = map(int, header.split())  # throws for invalid file format\n",
    "        result = Word2Vec(size=vector_size)\n",
    "        result.syn0 = zeros((vocab_size, vector_size), dtype=REAL)\n",
    "        if binary:\n",
    "            binary_len = dtype(REAL).itemsize * vector_size\n",
    "            for line_no in xrange(vocab_size):\n",
    "                # mixed text and binary: read text first, then binary\n",
    "                word = []\n",
    "                while True:\n",
    "                    ch = fin.read(1)\n",
    "                    if ch == b' ':\n",
    "                        break\n",
    "                    if ch != b'\\n':  # ignore newlines in front of words (some binary files have)\n",
    "\n",
    "                        word.append(ch)\n",
    "                try:\n",
    "                    word = utils.to_unicode(b''.join(word), encoding=encoding)\n",
    "                except UnicodeDecodeError, e:\n",
    "                    logger.warning(\"Couldn't convert whole word to unicode: trying to convert first %d bytes only ...\" % e.start)\n",
    "                    word = utils.to_unicode(b''.join(word[:e.start]), encoding=encoding)\n",
    "                    logger.warning(\"... first %d bytes converted to '%s'\" % (e.start, word))\n",
    "\n",
    "                if counts is None:\n",
    "                    result.vocab[word] = Vocab(index=line_no, count=vocab_size - line_no)\n",
    "                elif word in counts:\n",
    "                    result.vocab[word] = Vocab(index=line_no, count=counts[word])\n",
    "                else:\n",
    "                    logger.warning(\"vocabulary file is incomplete\")\n",
    "                    result.vocab[word] = Vocab(index=line_no, count=None)\n",
    "                result.index2word.append(word)\n",
    "                result.syn0[line_no] = fromstring(fin.read(binary_len), dtype=REAL)\n",
    "        else:\n",
    "            for line_no, line in enumerate(fin):\n",
    "                parts = utils.to_unicode(line[:-1], encoding=encoding).split(\" \")\n",
    "                if len(parts) != vector_size + 1:\n",
    "                    raise ValueError(\"invalid vector on line %s (is this really the text format?)\" % (line_no))\n",
    "                word, weights = parts[0], list(map(REAL, parts[1:]))\n",
    "                if counts is None:\n",
    "                    result.vocab[word] = Vocab(index=line_no, count=vocab_size - line_no)\n",
    "                elif word in counts:\n",
    "                    result.vocab[word] = Vocab(index=line_no, count=counts[word])\n",
    "                else:\n",
    "                    logger.warning(\"vocabulary file is incomplete\")\n",
    "                    result.vocab[word] = Vocab(index=line_no, count=None)\n",
    "                result.index2word.append(word)\n",
    "                result.syn0[line_no] = weights\n",
    "    logger.info(\"loaded %s matrix from %s\" % (result.syn0.shape, fname))\n",
    "    result.init_sims(norm_only)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим предобученные векторы word2vec (источник: https://github.com/nlpub/russe-evaluation/tree/master/russe/measures/word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vecs = load_vectors('C:\\\\Users\\\\al.nikolaev\\\\Desktop\\\\Avito\\\\word2vec\\\\rus100.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размерность каждого вектора 1x100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(word_vecs[u'кошка'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отберем векторы Word2Vec для слов, которые есть в словаре, остальные заменим случайными векторами (векторизуем словарь)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_weights = [np.array([word_vecs[w] if w in word_vecs else np.random.uniform(-0.25,0.25,100) for w in vocab_inv])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовим входные данные для модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодируем цифрами индекса слова в корпусе текстов, чтобы для них можно было по индексу получать векторы из векторизованного словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[vocab[word] for word in text] for text in texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Целевая переменная - номер категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train.category_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемешаем исходные данные перед обучением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построим сверточную нейронную сеть с помощью библиотеки keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Зададим параметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Параметры исходных данных\n",
    "sequence_length = 455 # длина каждого текста выровнена до 455\n",
    "embedding_dim = 100 # размерность векторов word2vec         \n",
    "\n",
    "# Гиперпараметры модели \n",
    "filter_sizes = (3, 4, 5)\n",
    "num_filters = 5\n",
    "dropout_prob = (0.5, 0.7)\n",
    "hidden_dims = 100\n",
    "\n",
    "# Параметры обучения\n",
    "batch_size = 50\n",
    "num_epochs = 100\n",
    "val_split = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим структуру нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, Convolution1D, MaxPooling1D\n",
    "\n",
    "# Создаем слои свертки и подвыборки, а также выходной слой\n",
    "graph_in = Input(shape=(sequence_length, embedding_dim))\n",
    "convs = []\n",
    "for fsz in filter_sizes:\n",
    "    conv = Convolution1D(nb_filter=num_filters,\n",
    "                         filter_length=fsz,\n",
    "                         border_mode='valid', # сужающая свертка\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(graph_in)\n",
    "    pool = MaxPooling1D()(conv)\n",
    "    flatten = Flatten()(pool)\n",
    "    convs.append(flatten)\n",
    "    \n",
    "out = Merge(mode='concat')(convs)\n",
    "graph = Model(input=graph_in, output=out)\n",
    "\n",
    "# Основная структура модели (последовательность слоев)\n",
    "model = Sequential()\n",
    "model.add(Embedding(embedding_weights[0].shape[0], embedding_dim, input_length=sequence_length, weights=embedding_weights))\n",
    "model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim))) # dropout отключает часть нейронов, \n",
    "# чтобы избежать их подстройки друг под друга и переобучения\n",
    "model.add(graph)\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(dropout_prob[1]))\n",
    "model.add(Activation('relu')) # функция активации ReLu max(0,x)\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid')) # функция активации сигмоида\n",
    "model.compile(loss='mean_absolute_error', optimizer='sgd', metrics=['accuracy']) # метрика качества accuracy\n",
    "# цель - минимизация средней абсолютной ошибки, т.к. близкие друг к другу категории чаще всего близки по id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_shuffled, y_shuffled, batch_size=batch_size, nb_epoch=num_epochs, validation_split=val_split, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Альтернативная, более простая модель - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание и реализация метода приведены в статье \"Supervised Learning for Document Classification with Scikit Learn\"\n",
    "https://www.quantstart.com/articles/Supervised-Learning-for-Document-Classification-with-Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Подготовим исходные данные: закодируем слова текстов методом TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При преобразовании TF-IDF вес слова пропорционален количеству употребления этого слова в документе и обратно пропорционален частоте употребления слова в других документах коллекции. Каждому документу будет соответствовать отдельная строка, каждому слову - отдельный столбец. Данных подход позволяет не учитывать фоновые, часто встречающиеся и не определяющие тему слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "x_tfidf = vectorizer.fit_transform([' '.join(text) for text in texts[shuffle_indices]])\n",
    "y_tfidf = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем модель SVM с радиальным ядром"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(C=1000000.0, gamma='auto', kernel='rbf') \n",
    "# C - штраф за помещение перемешанных объектов разных классов в промежуточную полосу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm.fit(x_tfidf, y_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества моделей на тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('C:\\\\Users\\\\al.nikolaev\\\\Desktop\\\\Avito\\\\data\\\\train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В функцию нормализации добавляем условие: оставлять только слова из тренировочного корпуса текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.decode('utf-8') # декодируем юникод\n",
    "    text = re.sub(r\"\\s+[^\\s]*[0-9]+[^\\s]*([\\s]+|$)\", \" \", text) # убираем модели, номера, измерения\n",
    "    text = re.sub(ur\"[^ЁёА-Яа-яA-Za-z\\s]\", \" \", text) # убираем все небуквенные символы\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text) # убираем лишние пробелы\n",
    "    text = text.strip() # убираем пробелы в начале и в конце\n",
    "    text = text.lower() # приводим все слова к строчным буквам\n",
    "    text = text.split(\" \") # разбиваем текст на слова\n",
    "    text = [morph.parse(word)[0].normal_form for word in text] # заменяем каждое слово его нормальной формой\n",
    "    text2 = []\n",
    "    for word in text:\n",
    "        if word in vocab: # добавляем только слова из словаря\n",
    "            text2.append(word)\n",
    "    text = text2\n",
    "    return text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    text = test.title[i] + ' ' + test.description[i] \n",
    "    texts.append(normalize_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = equalize_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array([[vocab[word] for word in text] for text in texts])\n",
    "x_tfidf_test = vectorizer.transform([' '.join(text) for text in texts)\n",
    "y_test = test.category_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_cnn = model.predict(x_test)\n",
    "y_svm = svm.predict(x_tfidf_test)\n",
    "\n",
    "cnn_score = accuracy_score(y_test, y_cnn)\n",
    "svm_score = accuracy_score(y_test, y_svm)\n",
    "\n",
    "print cnn_score, svm_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитаем точность по категориям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat = pd.read_csv('C:\\\\Users\\\\al.nikolaev\\\\Desktop\\\\Avito\\\\data\\\\category.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим столбцы с первым и вторым уровнем категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = []\n",
    "for t in cat.name:\n",
    "    h.append(t.split(\"|\"))\n",
    "    \n",
    "cat0, cat1 = [], []\n",
    "for i in range(len(h)):\n",
    "    cat0.append(h[i][0])\n",
    "    cat1.append(h[i][1])\n",
    "    \n",
    "cat_split = pd.DataFrame(zip(cat0, cat1))\n",
    "cat = pd.concat([cat, cat_split], axis=1)\n",
    "cat.columns = ['category_id', 'name', 'cat0', 'cat1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим датафреймы с истинными категориями тестовой выборки и предсказанными значениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_true = pd.DataFrame(y_test)\n",
    "df_cnn = pd.DataFrame(y_cnn)\n",
    "df_svm = pd.DataFrame(y_svm)\n",
    "\n",
    "df_true.columns = ['category_id']\n",
    "df_cnn.columns = ['category_id']\n",
    "df_svm.columns = ['category_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим их с таблицей категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_true = df_true.merge(cat, on='category_id', how='left')\n",
    "df_cnn = df_cnn.merge(cat, on='category_id', how='left')\n",
    "df_svm = df_svm.merge(cat, on='category_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим точность классификации по верхнему уровню категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc0_cnn = np.array([1 if a == b else 0 for a, b in zip(df_true.cat0, df_cnn.cat0)]).mean()\n",
    "acc0_svm = np.array([1 if a == b else 0 for a, b in zip(df_true.cat0, df_svm.cat0)]).mean()\n",
    "\n",
    "print acc0_cnn, acc0_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc1_cnn = np.array([1 if a == b else 0 for a, b in zip(df_true.cat1, df_cnn.cat1)]).mean()\n",
    "acc1_svm = np.array([1 if a == b else 0 for a, b in zip(df_true.cat1, df_svm.cat1)]).mean()\n",
    "\n",
    "print acc1_cnn, acc1_svm"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
